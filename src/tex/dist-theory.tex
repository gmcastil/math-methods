

\chapter{Distribution Theory}
\label{ch:distribution-theory}

\section{The Dirac $\delta$-Function}
\label{sec:delta-function}
The $\delta$-function can be defined in several different ways.  The
simplest is to say that
\begin{equation*}
  \delta(x) = \left\{
    \begin{array}{lr}
      0 & : x \neq 0\\
      \infty & : x = 0
    \end{array}
  \right.
\end{equation*}
with $\delta(0)$ being infinite in such a way that
\begin{equation*}
  \intab{a}{b}{x}\delta(x) = 1
\end{equation*}
for $a < 0 < b$ (since $\delta(x) = 0$ for $x \neq 0$ this integral is
independent of $a$ and $b$, so long as $a < 0 < b$).

A second possibility is to define $\delta(x)$ to be the limit as $n
\rightarrow \infty$ of a delta sequence $\delta_n(x)$ with the understanding
that, if there is an integral over $x$, the integration is to be performed
prior to taking the limit as $n \rightarrow \infty$.  There are many possible
delta sequences, one of which is
\begin{equation*}
  \delta_n(x) = \pi^{-\frac{1}{2}} n e^{-n^2x^2}
\end{equation*}
Defining $\delta(x) = \lim_{n \to \infty} \delta_n(x)$ yields
\begin{equation*}
  \delta(x) = \left\{
    \begin{array}{lr}
      0 & : x \neq 0\\
      \infty & : x = 0
    \end{array}
  \right.
\end{equation*}
as before.  In addition, we have (since the integral is to be done before
taking the limit)
\begin{align*}
  \intreal{x} \delta(x) &= \inflim{n} \intreal{x} \delta_n(x) \\
                        &= \inflim{n}
                          \invnroot{2}{\pi} n \intreal{x} e^{-n^2x^2} \\
                        &= \inflim{n}
                          \invnroot{2}{\pi} n
                          \left(\frac{\pi^{\frac{1}{2}}}{n}\right) \\
                        &= \inflim{n} \left(1\right) \\
                        &= 1
\end{align*}
and since $\delta(x) = 0$ for $x \neq 0$, this yields
\begin{equation*}
  \intab{a}{b}{x} \delta(x) = 1
\end{equation*}
whenever $a < 0 < b$, as before.  In fact, using delta sequences, we can show
that for any function $\phi(x)$, we have that
\begin{equation*}
  \intab{a}{b}{x} \phi(x) \delta(x) = \phi(0)
\end{equation*}
whenever $a < 0 < b$.

Finally, $\delta(x)$ can be defined by requiring that for any function
$\phi(x)$, we have that
\begin{equation*}
  \intreal{x} \phi(x)\delta(x) = \phi(0)
\end{equation*}
Furthermore, it can be shown that this yields
$\int_z^bdx\, \phi(x)\delta(x) = \phi(0)$ whenever $a < 0 < b$.

The first of these three definitions is rather intuitive, but it is not very
useful.  To show this, let us attempt to compute the derivative of the product
$f(x)\delta(x)$ for an arbitrary function $f(x)$.  We have
\begin{equation*}
  \ddx{x}\left[f(x)\delta(x)\right] = f'(x)\delta(x) + f(x)\delta'(x)
\end{equation*}
Since $\delta(x) = 0$ for $x \neq 0$, then we must also have $\delta'(x) = 0$
for $x \neq 0$.  Thus, this becomes
\begin{equation*}
  \ddx{x} \left[f(x)\delta(x)\right] = f'(0)\delta(x) + f(0)\delta'(x)
\end{equation*}
However, the same argument yields
\begin{equation*}
  f(x)\delta(x) = f(0)\delta(x)
\end{equation*}
and differentiating this gives
\begin{equation*}
  \ddx{x} \left[f(x)\delta(x)\right] = f(0)\delta'(x)
\end{equation*}
Clearly, something is wrong, but this simplistic definition is unable to
provide the answer.

The second definition in terms of delta sequences is better, but is also unable
to solve the above problem.  In factd, it yields
\begin{align*}
  \delta'(x) &= \inflim{n} \delta_n'(x) \\
             &= \inflim{n}
               \D{}{x} \left(\invnroot{2}{\pi} n e^{-n^2x^2}\right) \\
             &= \inflim{n} (-2\invnroot{2}{\pi} n^3 x e^{-n^2x^2} \\
             &= 0
\end{align*}
for all $x$.  However, we also have
\begin{align*}
  \delta''(x) &= \inflim{n} \delta_n''(x) \\
              &= \inflim{n} \D[2]{}{x}
                \left(\invnroot{2}{\pi} n e^{-n^2x^2}\right) \\
              &= \inflim{n} (-2 \invnroot{2}{\pi} n^3 e^{-n^2x^2}
                + 4 \invnroot{2}{\pi} n^5 x^2 e^{-n^2x^2}) \\
              &= \left\{\begin{array}{cr}
                   0       & : x \neq 0 \\
                   -\infty & : x = 0
                 \end{array}\right.
\end{align*}

But, if $\delta'(x) = 0$, we should have $\delta''(0) = 0$ also, so something
is again wrong.

The problem here is that the delta function is not really a function.  It
cannot be defined by specifying its value at all points.  The third definition
given earlier avoids this problem and is an acceptable definition.  We will
return to this definition in the next section.

\section{Distributions}
\label{sec:distributions}
As mentioned in the previous section, the delta function is not really a
function; it is a distribution.  A distribution is a functional - it is
specified by giving its values for all functions $\phi(x)$, rather than for all
points $x$.  More formally, the delta function is the distribution
\begin{equation*}
  \delta[\phi] = \phi(0)
\end{equation*}

Normal functions are special cases of distributions: as a distribution, a
function $f(x)$ is defined as
\begin{equation*}
  f[\phi] = \intreal{x} \phi(x) f(x)
\end{equation*}
Because of this, for a general distribution $D[\phi]$, one often uses the
notation
\begin{equation*}
  D[\phi] = \intreal{x} \phi(x) D(x)
\end{equation*}
and refers to the `function' $D(x)$.  In this notation, the above definition of
the delta function becomes
\begin{equation*}
  \intreal{x} \phi(x) \delta(x) = \phi(0)
\end{equation*}
which is just the third definition from Section~\ref{sec:delta-function}.  In
the next section, we will show how this definition solves the problem of
differentiating $f(x)\delta(x)$.  The main point of this section is that, for a
general distribution, $D(x)$ is defined only when an integral over $x$ is
present (i.e., only the quantity
\begin{equation*}
  D[\phi] = \intreal{x} \phi(x) D(x)
\end{equation*}
is really defined - $D(x)$ itself is not defined).
\section{Derivatives of Distributions}
A function $f(x)$ corresponds to the distribution
\begin{equation*}
  f[\phi] = \intreal{x} \phi(x) f(x)
\end{equation*}
Similarly, its derivative $f'(x)$ corresponds to the distribution
\begin{equation*}
  f'[\phi] = \intreal{x} \phi(x) f'(x)
\end{equation*}
Integrating by parts in this last equation yields
\begin{align*}
  f'[\phi] &= -\intreal{x} \phi'(x) f(x) \\
           &= -f[\phi']
\end{align*}
Note that we have assumed that $\phi(x) \rightarrow 0$ as $x$ approaches
$\pm \infty$.  This is because the class of functions $\phi(x)$ on which
distributions are defined must satisfy certain restrictions.  In particular,
$\phi(x)$ and all of its derivatives must exist, be continuous, and vanish as
$x$ approaches $\pm \infty$.

The above result motivates the definition of the derivative of a general
distribution as
\begin{equation*}
  D'[\phi] = -D[\phi']
\end{equation*}
or, in terms of the `function' $D(x)$,
\begin{equation*}
  \intreal{x} \phi(x) D'(x) = -\intreal{x} \phi'(x) D(x).
\end{equation*}
An important point is that, like the distribution itself, the derivative of a
general distribution is defined only inside of an integral.

We are now prepared to return to the problem of differentiating
$f(x)\delta(x)$.  By definition, or by integration by parts, we have
\begin{align*}
  \intreal{x} \phi(x) \D{}{x} \left[f(x)\delta(x)\right]
  &= -\intreal{x} \phi'(x) \left[f(x)\delta(x)\right] \\
  &= -\intreal{x} \left[f(x) \phi'(x)\right] \delta(x) \\
  &= -f(0) \phi'(0) \\
  &= -f(0) \intreal{x} \phi'(x) \delta(x) \\
  &= f(0) \intreal{x} \phi(x) \delta'(x) \\
  &= \intreal{x} \phi(x) \left[f(0) \delta'(x)\right]
\end{align*}
Since this must be true for any function $\phi(x)$ we must have
\begin{equation*}
  \D{}{x} \left[f(x)\delta(x)\right] = f(0)\delta'(x)
\end{equation*}
Thus the second calculation in Section~\ref{sec:delta-function} gave the
correct answer.  The problem with the first calculation in
Section~\ref{sec:delta-function} is that
$f(x)\delta'(x) \neq f(0)\delta'(x)$.  In fact, we have that
\begin{align*}
  \intreal{x} \phi(x)\left[f(x)\delta'(x)\right]
  &= \intreal{x} \left[f(x)\phi(x)\right] \delta'(x) \\
  &= -\intreal{x} \left[f'(x)\phi(x) + f(x)\phi'(x)\right] \delta(x) \\
  &= -\left[f'(0)\phi(0) + f(0)\phi'(0)\right] \\
  &= -f'(0) \intreal{x} \phi(x) \delta(x)
                 + f(0) \intreal{x} \phi(x) \delta'(x) \\
  &= \intreal{x} \phi(x) \left[f(0) \delta'(x) - f'(0)\delta(x)\right]
\end{align*}
Hence, we must have that
\begin{equation*}
  f(x)\delta'(x) = f(0)\delta'(x) - f'(0)\delta(x).
\end{equation*}
If this result is used in the first calculation of
Section~\ref{sec:delta-function}, the correct value is again obtained.

Note the following subtle point: $f(x)\delta'(x)$ depends only on the
properties of $f(x)$ at $x = 0$, but it depends on more than just the value of
$f(x)$ at $x = 0$.  The same is true for higher-order derivatives.  In fact, we
have that
\begin{equation*}
  f(x)\delta^{(n)}(x) = \sum_{m=0}^n \left(-1\right)^m \frac{n!}{m!(n-m)!}
                                              f^{(m)}(0) \delta^{(n-m)}(x)
\end{equation*}
\section{Derivatives of Delta Functions}
We now know that the delta function $\delta(x)$ is defined by the relation
\begin{equation*}
  \intreal{x} \phi(x) \delta(x) = \phi(0)
\end{equation*}
but what is meant by its derivative $\delta'(x)$?  By definition, or by
integration by parts, we have
\begin{align*}
  \intreal{x} \phi(x) \delta'(x) &= -\intreal{x} \phi'(x) \delta(x) \\
                                 &= -\phi'(0)
\end{align*}
As for the delta function itself, this relation defines the derivative
$\delta'(x)$.

For higher-order derivatives $\delta^{(n)}(x)$, repeated integration by parts
yields the following definition:
\begin{equation*}
  \intreal{x} \phi(x) \delta^{(n)}(x) = (-1)^n \phi^{(n)}(0)
\end{equation*}
\section{Discontinuous Functions}
Hhaving examined the derivative of the delta function, we now consider its
integral
\begin{equation*}
  \theta(x) = \intab{-\infty}{x}{z} \delta(z) =
  \left\{
    \begin{array}{lr}
      0 & : x < 0\\
      1 & : x > 0
    \end{array}
  \right.
\end{equation*}
($\theta(x)$ is not defined at $x = 0$).  This function $\theta(x)$ is known as
the theta function (or the Heaviside function).  Note that this function is
discontinuous, and that its derivative $\theta'(x) = \delta(x)$ is a delta
function.

This example illustrates a general property: the derivative of a discontinuous
function contains delta functions.  To see this, consider a more general
example:
\begin{equation*}
  f(x) = \left\{
    \begin{array}{lr}
      f_1(x) \quad x < x_0
      f_2(x) \quad x > x_0
    \end{array}\right.
\end{equation*}
where $f_1(x)$ and $f_2(x)$ are continuous and differentiable, but $f_1(x_0)
\neq f_2(x_0)$, so that $f(x)$ has a discontinuity at $x = x_0$. The derivative
of $f(x)$ is now found as follows:
\begin{align*}
  \intreal{x} \phi(x) f'(x)
      &= -\intreal{x} \phi'(x) f(x) \\
      &= -\intab{-\infty}{x_0}{x} \phi'(x) f_1(x)
      - \intab{x_0}{\infty}{x} \phi'(x) f_2(x) \\
      % This expression needs to be split properly
      &= -\left\{\left[\phi(x)f_1(x)\right]_{-\infty}^{x_0}
            -\intab{-\infty}{x_0}{x} \phi(x) f'_1(x)\right\}
         -\left\{\left[\phi(x)f_2(x)\right]_{x_0}^\infty
            -\intab{x_0}{\infty}{x} \phi(x) f'_2(x)\right\} \\
      % This expression needs to be split properly
      &= -\left[\phi(x_0)f_1(x_0) - 0\right]
            + \intab{-\infty}{x_0}{x} \phi(x) f'_1(x)
         - \left[0 - \phi(x_0)f_2(x_0)\right]
            + \intab{x_0}{\infty}{x} \phi(x) f'_2(x) \\
      % This expression needs to be split properly
      &= \intab{-\infty}{x_0}{x} \phi(x) f'_1(x)
        + \intab{x_0}{\infty}{x} \phi(x) f'_2(x)
        + \left[f_2(x_0) - f_1(x_0)\right] \phi(x_0)
\end{align*}
Letting
\begin{equation*}
  f'^{(p)}(x) = \left\{
    \begin{array}{lr}
      f'_1(x) &: x < x_0 \\
      f'_2(x) &: x > x_0
    \end{array}\right.
\end{equation*}
denote the piecewise derivative of $f(x)$ allos us to rewrite the above
expressions as
\begin{align*}
  \intreal{x} \phi(x) f'(x) &= \intreal{x} \phi(x) f'^{(p)}(x)
                              + \left[f_2(x_0) - f_1(x_0)\right]\phi(x_0) \\
                            &= \intreal{x} \phi(x) f'^{(p)}(x)
                              + \left[f_2(x_0) - f_1(x_0)\right]
                                \intreal{x} \phi(x)\delta(x - x_0) \\
                            &= \intreal{x} \phi(x)
                              \left\{f'^{(p)}(x)
                                + \left[f_2(x_0) - f_1(x_0)\right]
                                  \delta(x - x_0)\right\}
\end{align*}
Hence we have that
\begin{equation*}
  f'(x) = f'^{(p)}(x) + \left[f_2(x_0) - f_1(x_0)\right] \delta(x - x_0)
\end{equation*}

Thus, we see that the piecewise derivative is not the complete derivative.  The
complete derivative includes an additional delta function term.  In general,
the derivative of a discontinuous function is the sum of its piecwise
derivative and a delta function at each point of discontinuiuty multiplied by
the `jump' at that point.  This jump may be either positive or negative,
depending upon whether the function increases or decreases at the
discontinuity.

We may also obtain the above result in a different manner.  Using the theta
functions
\begin{equation*}
  \theta(x - x_0)
  = \left\{
    \begin{array}{lr}
      0 &\quad x-x_0 < 0 \\
      1 &\quad x-x_0 > 0
    \end{array}\right.
  = \left\{
    \begin{array}{lr}
      0 &\quad x < x_0 \\
      1 &\quad x > x_0
    \end{array}\right.
\end{equation*}
and
\begin{equation*}
  \theta(x_0 - x)
  = \left\{
    \begin{array}{lr}
      0 &\quad x_0 - x < 0 \\
      1 &\quad x_0 - x > 0
    \end{array}\right.
  = \left\{
    \begin{array}{lr}
      0 &\quad x > x_0 \\
      1 &\quad x < x_0
    \end{array}\right.
  = \left\{
    \begin{array}{lr}
      1 &\quad x < x_0 \\
      0 &\quad x > x_0
    \end{array}\right.
\end{equation*}
allows us to write
\begin{equation*}
  f(x)
  = \left\{
    \begin{array}{lr}
      f_1(x) &\quad x < x_0 \\
      f_2(x) &\quad x > x_0
    \end{array}\right.
  = \quad f_1(x)\theta(x_0 - x) + f_2(x)\theta(x - x_0)
\end{equation*}
and the piecewise derivative
\begin{equation*}
  f'^{(p)}(x)
  = \left\{
    \begin{array}{lr}
      f'_1(x) &\quad x < x_0 \\
      f'_2(x) &\quad x > x_0
    \end{array}\right.
  = \quad f'_1(x)\theta(x_0 - x) + f'_2(x)\theta(x - x_0)
\end{equation*}
Differentiating this expression for $f(x)$ yields
\begin{align*}
  f'(x) &= f'_1(x)\theta(x_0 - x) - f_1(x)\theta'(x_0 - x)
                   + f'_2(x)\theta(x - x_0) + f_2(x)\theta'(x - x_0) \\
        &= f'_1(x)\theta(x_0 - x) + f'_2\theta(x - x_0)
                   + f_2(x)\delta(x - x_0) - f_1(x)\delta(x_0 - x) \\
        &= f'^{(p)}(x) + f_2(x_0)\delta(x - x_0) - f_1(x_0)\delta(x - x_0) \\
        &= f'^{(p)}(x) + \left[f_2(x_0) - f_1(x_0)\right]\delta(x - x_0)
\end{align*}
as before.
\section{The Distribution $\delta\left[f(x)\right]$}
If $f(x)$ is a function with only first-order zeros, then we may define the
distribution $\delta\left[f(x)\right]$.  We begin by denoting the zeros of
$f(x)$ by $\left\{x_n\right\}$.  Since we must have
$\delta\left[f(x)\right] = 0$ unless $f(x) = 0$ (i.e., unless $x$ is one of the
zeros $x_n$), then $\delta\left[f(x)\right]$ must be a linear combination of
delta functions and their derivatives at the points $x_n$.  To find the
coefficients of these delta functions, we need only consider values of $x$
which are infinitesimally close to the zeros $x_n$.  Thus, we let
$x = x_n + \eps$ with $\eps$ infinitesimal.  Since $f(x_n) = 0$,l we have
(since $\eps$ is infinitesimal, we need only keep terms to lowest in order in
$\eps$
\begin{align*}
  f(x) &= f(x_n + \eps) \\
       &= f(x_n) + \eps f'(x_n) \\
       &= \eps f'(x_n)
\end{align*}
hence
\begin{equation*}
  \delta\left[f(x)\right] = \delta\left[\eps f'(x_n)\right]
\end{equation*}
Now, for $\alpha \neq 0$, we have (letting $y = \alpha x$)
\begin{align*}
  \intreal{x} \phi(x) \delta(\alpha x)
           &= \left\{
             \begin{array}{lr}
               \intreal{y} \left(\frac{1}{\alpha}\right)
                  \phi\left(\frac{y}{\alpha}\right) \delta(y) \quad \alpha > 0 \\
               \intab{\infty}{-\infty}{y} \left(\frac{1}{\alpha}\right)
                  \phi\left(\frac{y}{\alpha}\right) \delta(y) \quad \alpha < 0
             \end{array}\right. \\
           &= \left\{
             \begin{array}{lr}
               \frac{1}{\alpha} \intreal{y}
                 \phi\left(\frac{y}{\alpha}\right)\delta(y) \quad \alpha > 0 \\
               -\frac{1}{\alpha} \intreal{y}
                 \phi\left(\frac{y}{\alpha}\right)\delta(y) \quad \alpha < 0 \\
             \end{array}\right. \\
           &= \left\{
             \begin{array}{lr}
               \frac{1}{\alpha} \phi(0) \quad \alpha > 0 \\
               -\frac{1}{\alpha} \phi(0) \quad \alpha < 0
             \end{array}\right. \\
           &= -\frac{1}{\abs{\alpha}} \phi(0) \\
           &= \frac{1}{\abs{\alpha}} \intreal{x} \phi(x) \delta(x) \\
           &= \intreal{x} \phi(x) \left[\frac{1}{\abs{\alpha}} \delta(x)\right]
\end{align*}
hence
\begin{equation*}
  \delta(\alpha x) = \frac{1}{\abs{\alpha}} \delta(x)
\end{equation*}
so that the previous expression becomes
\begin{align*}
  \delta\left[f(x)\right] &= \delta\left[\eps f'(x_n)\right] \\
                          &= \frac{1}{\abs{f'(x_n)}} \delta(\eps) \\
                          &= \abs{f'(x_n)}^{-1} \delta(x - x_n)
\end{align*}
which is valid for $x$ infinitesimally close to the zero $x_n$.  Thus, the
complete expression (valid for all $x$) must be
\begin{equation*}
  \delta\left[f(x)\right] = \sum_n \abs{f'(x_n)}^{-1} \delta(x - x_n)
\end{equation*}

\section{Products of Functions and Delta Functions}
Consider the distribution
\begin{equation*}
  D_1(x) = x \delta(x)
\end{equation*}
Since
\begin{align*}
  \intreal{x} \phi(x) D_1(x) &= \intreal{x} \phi(x) \left[x \delta(x)\right] \\
                             &= \intreal{x} x \phi(x) \delta(x) \\
                             &= (0) \phi(0) \\
                             &= 0
\end{align*}
then this distribution $D_1(x)$ must be identically zero, hence
\begin{equation*}
  x\delta(x) = 0
\end{equation*}
More generally, we have that for any $\alpha > 0$
\begin{equation*}
  x^{\alpha} \delta(x) = 0
\end{equation*}

Net, consider the distribution
\begin{equation*}
  D_2(x) = x^{\alpha} \delta'(x)
\end{equation*}
Since,
\begin{align*}
  \intreal{x} \phi D_2(x)
        &= \intreal{x} x^{\alpha} \phi(x) \delta'(x) \\
        &= -\intreal{x} \left[\alpha x^{\alpha-1} \phi(x)
                        + x^{\alpha} \phi'(x))\right] \delta(x) \\
        &= -\left[\alpha x^{\alpha-1} \phi(x) + x^{\alpha} \phi'(x)\right]_{x = 0}
\end{align*}
which vanishes for $\alpha > 1$.  Thus, for $\alpha > 1$, we have
\begin{equation*}
  x^{\alpha}\delta'(x) = 0.
\end{equation*}
In general, we have
\begin{equation*}
  x^{\alpha}\delta^{(n)}(x) = 0
\end{equation*}
for $\alpha > n$.

Now, suppose that we want to find the solution $f(x)$ of the equation
\begin{equation*}
  x f(x) = 0
\end{equation*}
For $x \neq 0$, we can divide by $x$ to yield
\begin{equation*}
  f(x) = 0
\end{equation*}
but what about the point $x = 0$?  Since $f(x)$ vanishes for all $x \neq 0$,
then it must be a linear combination of the form
\begin{equation*}
  f(x) = \sum_{n=0}^\infty A_n \delta^{(n)}(x)
\end{equation*}
Multiplying by $x$ then yields
\begin{align*}
  x f(x) &= \sum_{n=0}^\infty A_n x \delta^{(n)}(x) \\
         &= \sum_{n=1}^\infty A_n x \delta^{(n)}(x)
\end{align*}
since $x \delta^{(n)}(x) = 0$ for $n < 1$ (i.e., for $n = 0$).  Since
$xf(x)=0$, then we must have $A_1 = A_2 = \dots = 0$, but $A_0$ is arbitrary.
Thus, the general solution of the equation
\begin{equation*}
  xf(x) = 0
\end{equation*}
is
\begin{equation*}
  f(x) = A \delta(x)
\end{equation*}
where $A$ is an arbitrary constant.

Next, consider the equation
\begin{equation*}
  xf(x) = g(x)
\end{equation*}
For $x \neq 0$, we have
\begin{equation*}
  f(x) = \frac{g(x)}{x}
\end{equation*}
but the general solution is
\begin{equation*}
  f(x) = \frac{g(x)}{x} + A \delta(x)
\end{equation*}
where $A$ is again arbitrary (this is true even if $\tfrac{g(x)}{x}$ is finite
at $x=0$).  More generally, the equation
\begin{equation*}
  x^\alpha f(x) = g(x)
\end{equation*}
has the general solution
\begin{equation*}
  f(x) = x^{-\alpha} g(x) + \sum_{n=0}^\alpha A_n \delta^{(n)}(x)
\end{equation*}
where the sum is over all nonnegative integers less than $\alpha$ (and the
$A_n$ are arbitrary constants).

Finally, for equations of the form
\begin{equation*}
  h(x) f(x) = g(x)
\end{equation*}
has the general solution
\begin{equation*}
  f(x) = \frac{g(x)}{h(x)} + \sum_m \sum_{n=0}^{N_m} A_{mn}\delta^{(n)}(x-x_m)
\end{equation*}
where $\{x_m\}$ are the zeros of $h(x)$, and $N_m$ is the order of the zero
$x_m$.  These extra pieces will be very important later when using Fourier
transforms to solve differential equations.
\section{Products of Distributions}
In general, distributions may be multiplied; however, there is one
restriction.  In order to understand this restriction, we must discuss the
singularities of distributions.  Basically, a singularity is a point where a
distribution or one of its derivatives is not well defined.  For example, the
delta function $\delta(x - a)$ and its derivatives $\delta^{(n)}(x - a)$ are
all singular at $x = a$.  Furthermore, all integrals of $\delta(x - a)$ are
singular at $x = a$, since some derivative of such an integral is $\delta(x -
a)$, which is singular (in particular, $\theta(x - a)$ is singular at $x =
a$).  Thus, a function is singular at any point where either function itself or
one of its derivatives is discontinuous.

The product of two distributions with no singularities in commons is always
well defined, but the product of two distributions with singularities at the
same point may or may not be well defined.  As an example, consider the
distribution $D_1(x) = \theta(x)\delta(x)$ which is defined by the relation
\begin{align*}
  \intreal{x} \phi(x)D_1(x) &= \intreal{x} \phi(x) \theta(x) \delta(x) \\
                            &= \phi(0) \theta(0)
\end{align*}
Since $\theta(0)$ is not well defined, then neither is
$D_1(x) = \theta(x)\delta(x)$.

As another example, consider $D_2(x) = \abs{x}\delta{x}$.  Since
\begin{align*}
  \intreal{x} \phi(x) D_2(x) &= \intreal{x} \abs{x} \phi(x) \delta(x) \\
                             &= \abs{0} \phi(0)
                             &= 0
\end{align*}
is well defined, then so is $D_2(x) = \abs{x}\delta(x)$.  On the other hand,
$D_3 = \abs{x} \delta'(x)$ satisfies
\begin{align*}
  \intreal{x} \phi(x) D_3(x) &= \intreal{x} \abs{x} \phi(x) \delta'(x) \\
                             &= -\intreal{x}
                               \delta(x) \D{}{x} \left[\abs{x}\phi(x)\right]
\end{align*}
Since $\abs{x}$ is continuous, then its derivative is just the piecewise
derivative, since there are no jumps
\begin{align*}
\D{}{x} \abs{x} &= \left\{
                  \begin{array}{lr}
                    \D{}{x} (x) & \quad x > 0 \\
                    \D{}{x} (-x) & \quad x < 0
                  \end{array}\right. \\
                &= \left\{
                  \begin{array}{lr}
                    1 & \quad x > 0 \\
                    -1 & \quad x < 0
                  \end{array}\right. \\
                &= \left\{
                  \begin{array}{lr}
                    2(1) - 1 \quad x > 0 \\
                    2(0) - 1 \quad x < 0
                  \end{array}\right. \\
                &= 2\theta(x) - 1
\end{align*}
Thus, we have that
\begin{align*}
  \intreal{x} \phi(x) D_3(x)
       &= -\intreal{x} \delta(x) \D{}{x} \left[\abs{x}\phi(x)\right] \\
       &= -\intreal{x} \delta(x) \left\{\left[2\theta(x) - 1\right]\phi(x)
                                         + \abs{x}\phi'(x)\right\} \\
       &= -\left[2\theta(0) - 1\right] \phi(0)
\end{align*}
Since $\theta(0)$ is not well defined, then neither is
$D_3(x) = \abs{x}\delta'(x)$.

Finally, if the product of two distributions with singularities at the same
point is well defined, differentiating this distribution may require some
care.  As an example, consider the distribution
\section{A Final Example}
In condensed-matter theory, one often encounters the quantity
\begin{equation*}
  D(x) = \theta(a + \eps - x)\theta(x - a)
\end{equation*}
with $\eps > 0$ (this quantity appears when considering the excitation of
electrons across an energy $\eps$ at zero temperature).  Since

\begin{align*}
  D(x) &= \left\{
         \begin{array}{lr}
           (0)(1) & : x > a + \eps \\
           (1)(1) & : a < x < a + \eps \\
           (1)(0) & : x < a
         \end{array}\right. \\
       &= \left\{
         \begin{array}{lr}
           1 & : a < x < a + \eps \\
           0 & : \textup{otherwise}
         \end{array}\right.
\end{align*}
then, as $\eps \to 0$, we have $D(x) \to 0$.  However, as $\eps \to 0$, what is
$D(x)$ to leading order in $\eps$?

Consider the Taylor expansion of $D(x)$
\begin{align*}
  D(x) &= \left[\theta(a - x) + \eps \theta'(a - x)\right] \theta(x - a) \\
       &= \theta(a - x) \theta(x - a) + \eps\delta(a - x)\theta(x - a)) \\
       &= 0 + \eps\delta(x - a)\theta(x - a) \\
       &= \eps\theta(x - a)\delta(x - a)
\end{align*}
which is not well defined.  However, we have that
\begin{align*}
  \intreal{x} \phi(x)D(x)
    &= \intreal{x} \phi(x) \theta(a + \eps - x)\theta(x - a) \\
    &= \intab{a}{\infty}{x} \phi(x) \theta(a + \eps - x) \\
    &= \intab{a}{a + \eps}{x} \phi(x) \\
    &= \eps \phi(a) \\
    &= \eps \intreal{x} \phi(x) \delta(x - a)
\end{align*}
hence $D(x) = \eps\delta(x - a)$ to leading order in $\eps$ as $\eps \to 0$.
%%% Local Variables:
%%% TeX-master: "body"
%%% End:
